{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 引言\n",
    "\n",
    "在本次IR实验中，请运用所学的知识，为搜索引擎构建一个简单的索引和检索系统，包括：\n",
    "\n",
    "1. [构建非压缩索引及检索实现](#构建非压缩索引及检索实现) 在stanford.edu网页语料库上构建未压缩索引，并实现布尔查询检索。\n",
    "2. [构建压缩索引](#构建压缩索引) 使用可变长度编码在同一个语料库上构建压缩索引，并实现布尔查询。\n",
    "3. [附加题](#其他压缩方法实现(选做))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 实验要求及提交说明\n",
    "\n",
    "1\\. 本次实验将于学期结束前提交，具体时间将提前至少2周发布。\n",
    "\n",
    "2\\. 实验项目中包括：\n",
    "\n",
    "**本实验文档PA1-skeleton.ipynb**\n",
    "\n",
    "**实验环境安装说明README.md**\n",
    "\n",
    "**测试语料库文件夹toy-data**\n",
    "\n",
    "**对完整语料库的查询文件dev_queries**\n",
    "\n",
    "**完整的语料库压缩文件**\n",
    "\n",
    "**其他配置文件**\n",
    "\n",
    "实验推荐安装Anaconda平台（python3版本），具体安装方法请参考网上教程。\n",
    "\n",
    "2\\. 实验需要提交内容包括：\n",
    "\n",
    "**一份实验报告**（文件名：姓名_学号）\n",
    "\n",
    "**通过dev_queries文件夹内的查询得到的查询结果文件，命名为dev_out，文件夹内命名规则与dev_queries保持一致**\n",
    "\n",
    "**实验项目所有内容的压缩文件**（注意：不要任意修改文件名，便于助教进行测试）\n",
    "\n",
    "其中，实验报告的内容需要包括以下内容：\n",
    "\n",
    "(1) 需要补充的关键代码及说明；\n",
    "\n",
    "(2) 所有存在输出结果的结果输出截图（包括测试部分）；\n",
    "\n",
    "(3) （选做）附加部分的代码说明及分析；\n",
    "\n",
    "(4) 实验总结。\n",
    "\n",
    "3\\. 请将以上所有提交内容打包到一个压缩文件中，命名为IREXP_姓名_学号，发送至邮箱：irdm_ustc@163.com 。\n",
    "\n",
    "4\\. 本实验参考Stanfod相关课程实验，故以下内容以英文为主。由于本学年第一次新增IR部分实验，如对实验内容有任何问题，欢迎与助教讨论或在QQ群内讨论，助教邮箱：gxq13@mail.ustc.edu.cn 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the tee magic which saves a copy of the cell when executed\n",
    "%reload_ext autograding_magics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `submission` folder will contain all the files to be submitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try: \n",
    "    os.mkdir('submission')\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting submission/imports.py\n"
     ]
    }
   ],
   "source": [
    "%%tee submission/imports.py\n",
    "\n",
    "# You can add additional imports here\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import array\n",
    "import os\n",
    "import timeit\n",
    "import contextlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus （语料库）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus you will be working with for this assignment contains webpages from the stanford.edu domain. The data for this assignment is available as a .zip file at: http://web.stanford.edu/class/cs276/pa/pa1-data.zip. The following code puts the data folder under the current directory. The total size of the corpus is about 170MB.\n",
    "（该文件已提前下载在实验文件中）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-89f35f6e7c27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# urllib.request.urlretrieve(data_url, data_dir+'.zip')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mzip_ref\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.zip'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mzip_ref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\zipfile.py\u001b[0m in \u001b[0;36mextractall\u001b[1;34m(self, path, members, pwd)\u001b[0m\n\u001b[0;32m   1614\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1615\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mzipinfo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1616\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extract_member\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzipinfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1617\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1618\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\zipfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[1;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[0;32m   1668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1669\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpwd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1670\u001b[1;33m              \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargetpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1671\u001b[0m             \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1672\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "data_url = 'http://web.stanford.edu/class/cs276/pa/pa1-data.zip'\n",
    "data_dir = 'pa1-data'\n",
    "# urllib.request.urlretrieve(data_url, data_dir+'.zip')\n",
    "zip_ref = zipfile.ZipFile(data_dir+'.zip', 'r')\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index will be stored in `output_dir` and `tmp` will contain some temporary files for toy indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    os.mkdir('output_dir')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "try: \n",
    "    os.mkdir('tmp')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "try: \n",
    "    os.mkdir('toy_output_dir')\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 10 sub-directories (named 0-9) under the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(os.listdir('pa1-data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each file under the sub-directory is the content of an individual webpage. You should assume that the individual file names are unique within each sub-directory, but not necessarily the case across sub-directories (*i.e.,* the full path of the files are unique)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3dradiology.stanford.edu_',\n",
       " '3dradiology.stanford.edu_patient_care_Case%2520studies_AVM.html',\n",
       " '3dradiology.stanford.edu_patient_care_case_studies.html',\n",
       " '5-sure.stanford.edu_',\n",
       " '50years.stanford.edu_',\n",
       " 'a3cservices.stanford.edu_awards_nominate_',\n",
       " 'a3cservices.stanford.edu_facilities_',\n",
       " 'a3cservices.stanford.edu_lead_',\n",
       " 'aa.stanford.edu_',\n",
       " 'aa.stanford.edu_about_aviation.php']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(os.listdir('pa1-data/0'))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the HTML info has been stripped off from those webpages, so they only contain space-delimited words. You should not re-tokenize the words. Each consecutive span of non-space characters consists of a word token in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3d radiology lab stanford university school of medicine stanford school of medicine 3d and quantitative imaging in the department of radiology search this site only stanford medical sites ways to give find a person alumni lane library ways to give find a person about us mission to develop and apply innovative techniques for efficient quantitative analysis and display of medical imaging data through interdisciplinary collaboration goals education to train physicians and technologists locally and worldwide in the latest developments in 3d and quantitative imaging research to develop new approaches to the exploration analysis and quantitative assesment of diagnostic images that result in a new and or more cost effective diagnostic approaches and b new techniques for the design and planning and monitoring of therapy patient care to deliver valid clinically relevant visualization and analysis of medical imaging data to the stanford community locations richard m lucas magnetic resonance imaging center 1201 welch rd p170 stanford ca 94305 5488 650 725 8432 james h clark center 318 campus drive s344 stanford ca 94305 5450 650 725 6862 directions to the 3qd lab local hotels you are here stanford medicine school of medicine departments radiology 3dq laboratory navigation for this section 3dq laboratory home education research patient care industry about us site navigation home education overview visiting fellowships reading list research overview research activities research opportunities patient care overview for physicians for patients protocol development 3dq management software case studies testimonials industry overview infrastructure and services about us faculty and staff history of the lab resources and equipment contact information jobs getting to the 3dq lab stanford medicine resources stanford medicine getting care overview find a physician find a clinical care center stanford hospital & clinics lucile packard children's hospital emergency research overview school of medicine news & resources clinical trials departments institutes & centers faculty profiles education & training overview school of medicine programs admissions continuing medical education alumni lane medical library community overview stanford health library community newsletter volunteering public service & community partnerships renewal & new building projects about us overview news careers ways to give find a person contact us maps & directions the dean's newsletter stanford university footer links contact us directions members only 2009 stanford school of medicine terms of use powered by irt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('pa1-data/0/3dradiology.stanford.edu_', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also find a small corpus included with the assignment in the folder `toy-data`. We will use this toy dataset to test our code before running on the complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_dir = 'toy-data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建非压缩索引及检索实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of this assignment is to build an inverted index of this corpus, and implement Boolean conjunctive queries. **In particular, you need to implement the blocked sort-based indexing (BSBI) algorithm described in [Section 4.2](http://nlp.stanford.edu/IR-book/pdf/04const.pdf) of the textbook.** While we quote some fundamentals from the textbook, we strongly suggest that you read Section 4.2 in more detail to fully understand the BSBI algorithm.\n",
    "\n",
    "实验的第一部分是建立这个语料库的倒排索引，并实现布尔查询（包括实现教材4.2节中描述的BSBI算法）。\n",
    "\n",
    "\n",
    "> To construct an index, we first make a pass through the collection assembling all term-docID pairs. We then sort the pairs with the term as the dominant key and docID as the secondary key. Finally, we organize the docIDs for each term into a postings list and compute statistics like term and document frequency. For small collections, all this can be done in memory. \n",
    "\n",
    "In this assignment, you will be implementing methods for large collections that require the use of secondary storage (*i.e.,* disk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IdMap\n",
    "\n",
    "Again quoting from Section 4.2:\n",
    "\n",
    "> To make index construction more efficient, we represent terms as termIDs (instead of strings), where each termID is a unique serial number. We can build the mapping from terms to termIDs on the fly while we are processing the collection. Similarly, we also represent documents as docIDs (instead of strings).\n",
    "\n",
    "Let us first build a helper class `IdMap`, which maps strings to numeric ids (and vice-versa). We will need this to bijectively map term to termID and doc to docID.\n",
    "\n",
    "Fill in the functions `_get_str` and `_get_id` in the following code. The only interface to this class is provided by `__getitem__` which gets the correct mapping depending on the type of the key. (See [these docs](https://docs.python.org/3.7/reference/datamodel.html#special-method-names) if you are unfamiliar with special functions like `__getitem__` ). Further, to simplify code later on, we'll also add some logic to add a string if it doesn't already exist in the map.   \n",
    "\n",
    "We will use a dictionary for efficient access from strings to numeric ids, and we will use a list for efficient access (and storage) from numeric ids to strings.\n",
    "\n",
    "You might also find this [short tutorial](https://www.omkarpathak.in/2018/04/11/python-getitem-and-setitem/) useful to get started with special (a.k.a magic) methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting submission/id_map.py\n"
     ]
    }
   ],
   "source": [
    "%%tee submission/id_map.py\n",
    "class IdMap:\n",
    "    \"\"\"Helper class to store a mapping from strings to ids.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.str_to_id = {}\n",
    "        self.id_to_str = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return number of terms stored in the IdMap\"\"\"\n",
    "        return len(self.id_to_str)\n",
    "        \n",
    "    def _get_str(self, i):\n",
    "        \"\"\"Returns the string corresponding to a given id (`i`).\"\"\"\n",
    "        ### Begin your code\n",
    "        if i > len(self.id_to_str) - 1:\n",
    "            pass\n",
    "        return self.id_to_str[i]\n",
    "        ### End your code\n",
    "        \n",
    "    def _get_id(self, s):\n",
    "        \"\"\"Returns the id corresponding to a string (`s`). \n",
    "        If `s` is not in the IdMap yet, then assigns a new id and returns the new id.\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        if s not in self.id_to_str:\n",
    "            self.id_to_str.append(s)\n",
    "            self.str_to_id[s] = len(self.id_to_str) - 1\n",
    "        return self.str_to_id[s]\n",
    "        ### End your code\n",
    "            \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"If `key` is a integer, use _get_str; \n",
    "           If `key` is a string, use _get_id;\"\"\"\n",
    "        if type(key) is int:\n",
    "            return self._get_str(key)\n",
    "        elif type(key) is str:\n",
    "            return self._get_id(key)\n",
    "        else:\n",
    "            raise TypeError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure it passes the following simple test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testIdMap = IdMap()\n",
    "assert testIdMap['a'] == 0, \"Unable to add a new string to the IdMap\"\n",
    "assert testIdMap['bcd'] == 1, \"Unable to add a new string to the IdMap\"\n",
    "assert testIdMap['a'] == 0, \"Unable to retrieve the id of an existing string\"\n",
    "assert testIdMap[1] == 'bcd', \"Unable to retrive the string corresponding to a\\\n",
    "                                given id\"\n",
    "try:\n",
    "    testIdMap[2]\n",
    "except IndexError as e:\n",
    "    assert True, \"Doesn't throw an IndexError for out of range numeric ids\"\n",
    "assert len(testIdMap) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going forward we will expect you to write your own test cases to make sure parts of your program are working as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Postings List as bytearrays\n",
    "\n",
    "In order to write and read lists of postings (docIDs) efficiently from the disk, we store them as bytearrays. We've provided an implementation of `UncompressedPostings` class which contains static encode and decode functions. In the next task you'll be required to implement compressed versions with the same inferface. \n",
    "\n",
    "For reference:\n",
    "1. https://docs.python.org/3/library/array.html\n",
    "2. https://pymotw.com/3/array/#module-array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UncompressedPostings:\n",
    "    \n",
    "    @staticmethod\n",
    "    def encode(postings_list):\n",
    "        \"\"\"Encodes postings_list into a stream of bytes\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        postings_list: List[int]\n",
    "            List of docIDs (postings)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        bytes\n",
    "            bytearray representing integers in the postings_list\n",
    "        \"\"\"\n",
    "        return array.array('L', postings_list).tobytes()\n",
    "        \n",
    "    @staticmethod\n",
    "    def decode(encoded_postings_list):\n",
    "        \"\"\"Decodes postings_list from a stream of bytes\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        encoded_postings_list: bytes\n",
    "            bytearray representing encoded postings list as output by encode \n",
    "            function\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        List[int]\n",
    "            Decoded list of docIDs from encoded_postings_list\n",
    "        \"\"\"\n",
    "        \n",
    "        decoded_postings_list = array.array('L')\n",
    "        decoded_postings_list.frombytes(encoded_postings_list)\n",
    "        return decoded_postings_list.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate how it works, run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x03\\x00\\x00\\x00'\n",
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "x = UncompressedPostings.encode([1,2,3])\n",
    "print(x)\n",
    "print(UncompressedPostings.decode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted Index on Disk\n",
    "\n",
    "> With main memory insufficient, we need to use an external sorting algorithm, that is, one that uses disk. For acceptable speed, the central requirement of such an algorithm is that it minimize the number of random disk seeks during sorting - sequential disk reads are far faster than seeks. \n",
    "\n",
    "In this section we provide a base class `InvertedIndex` which would be subsequently subclassed into `InvertedIndexWriter`, `InvertedIndexIterator` and `InvertedIndexMapper`. While typically `cPickle` is used for serialization in Python, it does not support partial reads and writes, which is why we are rolling out our own custom solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndex:\n",
    "    \"\"\"A class that implements efficient reads and writes of an inverted index \n",
    "    to disk\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    postings_dict: Dictionary mapping: termID->(start_position_in_index_file, \n",
    "                                                number_of_postings_in_list,\n",
    "                                               length_in_bytes_of_postings_list)\n",
    "        This is a dictionary that maps from termIDs to a 3-tuple of metadata \n",
    "        that is helpful in reading and writing the postings in the index file \n",
    "        to/from disk. This mapping is supposed to be kept in memory. \n",
    "        start_position_in_index_file is the position (in bytes) of the postings \n",
    "        list in the index file\n",
    "        number_of_postings_in_list is the number of postings (docIDs) in the \n",
    "        postings list\n",
    "        length_in_bytes_of_postings_list is the length of the byte \n",
    "        encoding of the postings list\n",
    "    \n",
    "    terms: List[int]\n",
    "        A list of termIDs to remember the order in which terms and their \n",
    "        postings lists were added to index. \n",
    "        \n",
    "        After Python 3.7 we technically no longer need it because a Python dict \n",
    "        is an OrderedDict, but since it is a relatively new feature, we still\n",
    "        maintain backward compatibility with a list to keep track of order of \n",
    "        insertion. \n",
    "    \"\"\"\n",
    "    def __init__(self, index_name, postings_encoding=None, directory=''):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        index_name (str): Name used to store files related to the index\n",
    "        postings_encoding: A class implementing static methods for encoding and \n",
    "            decoding lists of integers. Default is None, which gets replaced\n",
    "            with UncompressedPostings\n",
    "        directory (str): Directory where the index files will be stored\n",
    "        \"\"\"\n",
    "\n",
    "        self.index_file_path = os.path.join(directory, index_name+'.index')\n",
    "        self.metadata_file_path = os.path.join(directory, index_name+'.dict')\n",
    "\n",
    "        if postings_encoding is None:\n",
    "            self.postings_encoding = UncompressedPostings\n",
    "        else:\n",
    "            self.postings_encoding = postings_encoding\n",
    "        self.directory = directory\n",
    "\n",
    "        self.postings_dict = {}\n",
    "        self.terms = []         #Need to keep track of the order in which the \n",
    "                                #terms were inserted. Would be unnecessary \n",
    "                                #from Python 3.7 onwards\n",
    "\n",
    "    def __enter__(self):\n",
    "        \"\"\"Opens the index_file and loads metadata upon entering the context\"\"\"\n",
    "        # Open the index file\n",
    "        self.index_file = open(self.index_file_path, 'rb+')\n",
    "\n",
    "        # Load the postings dict and terms from the metadata file\n",
    "        with open(self.metadata_file_path, 'rb') as f:\n",
    "            self.postings_dict, self.terms = pkl.load(f)\n",
    "            self.term_iter = self.terms.__iter__()                       \n",
    "\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exception_type, exception_value, traceback):\n",
    "        \"\"\"Closes the index_file and saves metadata upon exiting the context\"\"\"\n",
    "        # Close the index file\n",
    "        self.index_file.close()\n",
    "        \n",
    "        # Write the postings dict and terms to the metadata file\n",
    "        with open(self.metadata_file_path, 'wb') as f:\n",
    "            pkl.dump([self.postings_dict, self.terms], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are interacting with a file on disk, we provide `__enter__` and `__exit__` functions, which allow using the `with` statement just like you would with file IO in python (Refer to the [official documentation for context managers](https://docs.python.org/3/library/contextlib.html)).\n",
    "\n",
    "Here is an example of how to use the `InvertedIndexWriter` context manager:\n",
    "\n",
    "```python\n",
    "with InvertedIndexWriter('test', directory='tmp/') as index:\n",
    "    # Some code here\n",
    "```\n",
    "\n",
    "If you are unfamiliar with context managers in python, we would recommend checking out these tutorials ([1](https://jeffknupp.com/blog/2016/03/07/python-with-context-managers/), [2](http://arnavk.com/posts/python-context-managers/)), although it isn't necessary to be able to understand all of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "> BSBI (i) segments the collection into parts of equal size, (ii) sorts the termID-docID pairs of each part in memory, (iii) stores intermediate sorted results on disk, and (iv) merges all intermediate results into the final index. \n",
    "\n",
    "You should treat each sub-directory as a block, and only load one block in memory at a time when you build your index (note: you will be penalized if you construct the index by loading blocks of larger than one directory in memory at once). Note that we are abstracting away from the operating systems meaning of blocks. You can assume that each block is small enough to be stored in memory.\n",
    "\n",
    "In this section, we introduce the class `BSBIIndex` which we will progressively build. The `index` function provides the skeleton for BSBI and your job is to implement `parse_block`, `invert_write` and `merge` functions in subsequent sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Do not make any changes here, they will be overwritten while grading\n",
    "class BSBIIndex:\n",
    "    \"\"\" \n",
    "    Attributes\n",
    "    ----------\n",
    "    term_id_map(IdMap): For mapping terms to termIDs\n",
    "    doc_id_map(IdMap): For mapping relative paths of documents (eg \n",
    "        0/3dradiology.stanford.edu_) to docIDs\n",
    "    data_dir(str): Path to data\n",
    "    output_dir(str): Path to output index files\n",
    "    index_name(str): Name assigned to index\n",
    "    postings_encoding: Encoding used for storing the postings.\n",
    "        The default (None) implies UncompressedPostings\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, output_dir, index_name = \"BSBI\", \n",
    "                 postings_encoding = None):\n",
    "        self.term_id_map = IdMap()\n",
    "        self.doc_id_map = IdMap()\n",
    "        self.data_dir = data_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.index_name = index_name\n",
    "        self.postings_encoding = postings_encoding\n",
    "\n",
    "        # Stores names of intermediate indices\n",
    "        self.intermediate_indices = []\n",
    "        \n",
    "    def save(self):\n",
    "        \"\"\"Dumps doc_id_map and term_id_map into output directory\"\"\"\n",
    "        \n",
    "        with open(os.path.join(self.output_dir, 'terms.dict'), 'wb') as f:\n",
    "            pkl.dump(self.term_id_map, f)\n",
    "        with open(os.path.join(self.output_dir, 'docs.dict'), 'wb') as f:\n",
    "            pkl.dump(self.doc_id_map, f)\n",
    "    \n",
    "    def load(self):\n",
    "        \"\"\"Loads doc_id_map and term_id_map from output directory\"\"\"\n",
    "        \n",
    "        with open(os.path.join(self.output_dir, 'terms.dict'), 'rb') as f:\n",
    "            self.term_id_map = pkl.load(f)\n",
    "        with open(os.path.join(self.output_dir, 'docs.dict'), 'rb') as f:\n",
    "            self.doc_id_map = pkl.load(f)\n",
    "            \n",
    "    def index(self):\n",
    "        \"\"\"Base indexing code\n",
    "        \n",
    "        This function loops through the data directories, \n",
    "        calls parse_block to parse the documents\n",
    "        calls invert_write, which inverts each block and writes to a new index\n",
    "        then saves the id maps and calls merge on the intermediate indices\n",
    "        \"\"\"\n",
    "        for block_dir_relative in sorted(next(os.walk(self.data_dir))[1]):\n",
    "            print(block_dir_relative)\n",
    "            td_pairs = self.parse_block(block_dir_relative)\n",
    "            print(td_pairs)\n",
    "            index_id = 'index_'+block_dir_relative\n",
    "            self.intermediate_indices.append(index_id)\n",
    "            with InvertedIndexWriter(index_id, directory=self.output_dir, \n",
    "                                     postings_encoding=\n",
    "                                     self.postings_encoding) as index:\n",
    "                self.invert_write(td_pairs, index)\n",
    "                td_pairs = None\n",
    "        self.save()\n",
    "        with InvertedIndexWriter(self.index_name, directory=self.output_dir, \n",
    "                                 postings_encoding=\n",
    "                                 self.postings_encoding) as merged_index:\n",
    "            with contextlib.ExitStack() as stack:\n",
    "                indices = [stack.enter_context(\n",
    "                    InvertedIndexIterator(index_id, \n",
    "                                          directory=self.output_dir, \n",
    "                                          postings_encoding=\n",
    "                                          self.postings_encoding)) \n",
    "                 for index_id in self.intermediate_indices]\n",
    "                self.merge(indices, merged_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing\n",
    "\n",
    "> The function `parse_block`  parses documents into termID-docID pairs and accumulates the pairs in memory until a block of a fixed size is full. We choose the block size to fit comfortably into memory to permit a fast in-memory sort. \n",
    "\n",
    "You should treat each sub-directory as a block; `parse_block` expects the path to the sub-directory which is the block. You should assume that the individual file names are unique within each sub-directory, but not necessarily the case across sub- directories (i.e., the full path of the files are unique).\n",
    "\n",
    "_NB - We are inheriting `BSBIIndex` from `BSBIIndex`, which is just a simple way of adding a new method to an existing class. Please do not be confused, it is simply used as a pedagogical tool to split a class definition within a jupyter notebook and there is no other magic happening here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting submission/parse_block.py\n"
     ]
    }
   ],
   "source": [
    "%%tee submission/parse_block.py\n",
    "class BSBIIndex(BSBIIndex):            \n",
    "    def parse_block(self, block_dir_relative):\n",
    "        \"\"\"Parses a tokenized text file into termID-docID pairs\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        block_dir_relative : str\n",
    "            Relative Path to the directory that contains the files for the block\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        List[Tuple[Int, Int]]\n",
    "            Returns all the td_pairs extracted from the block\n",
    "        \n",
    "        Should use self.term_id_map and self.doc_id_map to get termIDs and docIDs.\n",
    "        These persist across calls to parse_block\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        td_pairs = []\n",
    "        block_to_parse = os.path.join(self.data_dir, block_dir_relative)\n",
    "        docs = os.listdir(block_to_parse)\n",
    "        parsed = 0\n",
    "        for doc in docs:\n",
    "            doc = os.path.join(block_to_parse, doc)\n",
    "            parsed = parsed + 1\n",
    "            if parsed % 1000 == 0:\n",
    "                print(parsed)\n",
    "            doc_id = self.doc_id_map[doc]\n",
    "            td_pairs.extend(self._parse_file(doc, doc_id))\n",
    "        return td_pairs\n",
    "        ### End your code\n",
    "        \n",
    "    def _parse_file(self, doc, doc_id):\n",
    "        td_pair = []\n",
    "        with open(doc, 'r') as fp:\n",
    "            content = fp.read()\n",
    "        import re\n",
    "        content = re.sub('[?!,.]', '', content)\n",
    "        content = re.sub('\\s+', ' ', content)\n",
    "        content = content.strip().split(' ')\n",
    "        for term in content:\n",
    "            term_id = self.term_id_map[term]\n",
    "            td_pair.append((term_id, doc_id))\n",
    "        return td_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if the function works as expected on the toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i'm fine , thank you\n",
      "\n",
      "hi hi\n",
      "how are you ?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('toy-data/0/fine.txt', 'r') as f:\n",
    "    print(f.read())\n",
    "with open('toy-data/0/hello.txt', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (1, 0), (2, 0), (3, 0), (4, 1), (4, 1), (5, 1), (6, 1), (3, 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BSBI_instance = BSBIIndex(data_dir=toy_dir, output_dir = 'tmp/', index_name = 'toy')\n",
    "BSBI_instance.parse_block('0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the `parse_block` method work as expected on a block of the toy data? Write some tests to make sure that is indeed the case (for example, you should make sure that a given word gets the same id each time it appears)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7, 2), (8, 2), (9, 2), (3, 2), (10, 2), (9, 2), (3, 2), (11, 2), (10, 3), (10, 3), (10, 3)] [(0, 4), (1, 4), (2, 4), (3, 4), (4, 5), (4, 5), (5, 5), (6, 5), (3, 5)]\n"
     ]
    }
   ],
   "source": [
    "### Begin your code\n",
    "t1 = BSBI_instance.parse_block('1')\n",
    "t2 = BSBI_instance.parse_block('2')\n",
    "print(t1, t2)\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inversion\n",
    "\n",
    "> The block is then inverted and written to disk. Inversion involves two steps. First, we sort the termID-docID pairs. Next, we collect all termID-docID pairs with the same termID into a postings list, where a posting is simply a docID. The result, an inverted index for the block we have just read, is then written to disk.\n",
    "\n",
    "In this section we add the function `invert_write` which does exactly this. \n",
    "\n",
    "However, we first need to implement the class `InvertedIndexWriter`. This class provides an append function, just like a list, however the postings_list isn't stored in memory but is directly written to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting submission/inverted_index_writer.py\n"
     ]
    }
   ],
   "source": [
    "%%tee submission/inverted_index_writer.py\n",
    "class InvertedIndexWriter(InvertedIndex):\n",
    "    \"\"\"\"\"\"\n",
    "    def __enter__(self):\n",
    "        self.index_file = open(self.index_file_path, 'wb+')              \n",
    "        return self\n",
    "\n",
    "    def append(self, term, postings_list):\n",
    "        \"\"\"Appends the term and postings_list to end of the index file.\n",
    "        \n",
    "        This function does three things, \n",
    "        1. Encodes the postings_list using self.postings_encoding\n",
    "        2. Stores metadata in the form of self.terms and self.postings_dict\n",
    "           Note that self.postings_dict maps termID to a 3 tuple of \n",
    "           (start_position_in_index_file, \n",
    "           number_of_postings_in_list, \n",
    "           length_in_bytes_of_postings_list)\n",
    "        3. Appends the bytestream to the index file on disk\n",
    "\n",
    "        Hint: You might find it helpful to read the Python I/O docs\n",
    "        (https://docs.python.org/3/tutorial/inputoutput.html) for\n",
    "        information about appending to the end of a file.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        term:\n",
    "            term or termID is the unique identifier for the term\n",
    "        postings_list: List[Int]\n",
    "            List of docIDs where the term appears\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        if self.terms == []:\n",
    "            start_position_in_index_file = 0\n",
    "        else:\n",
    "            start_position_in_index_file = self.postings_dict[self.terms[-1]][0] + self.postings_dict[self.terms[-1]][2]\n",
    "        number_of_postings_in_list = len(postings_list)\n",
    "        postings_list = self.postings_encoding.encode(postings_list)\n",
    "        length_in_bytes_of_postings_list = len(postings_list)\n",
    "        self.postings_dict[term] = (start_position_in_index_file, number_of_postings_in_list, length_in_bytes_of_postings_list)\n",
    "        self.terms.append(term)\n",
    "        # content = self.index_file.read() + postings_list\n",
    "        # print(self.index_file.read(), self.postings_encoding.decode(content))\n",
    "        self.index_file.write(postings_list)\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we haven't written the classes which read the index, we can still test the implementation as follows. Make sure you pass the asserts before moving forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with InvertedIndexWriter('test', directory='tmp/') as index:\n",
    "    index.append(1, [2, 3, 4])\n",
    "    index.append(2, [3, 4, 5])\n",
    "    index.index_file.seek(0)\n",
    "    assert index.terms == [1,2], \"terms sequence incorrect\"\n",
    "    assert index.postings_dict == {1: (0, 3, len(UncompressedPostings.encode([2,3,4]))), \n",
    "                                   2: (len(UncompressedPostings.encode([2,3,4])), 3, \n",
    "                                       len(UncompressedPostings.encode([3,4,5])))}, \"postings_dict incorrect\"\n",
    "    assert UncompressedPostings.decode(index.index_file.read()) == [2, 3, 4, 3, 4, 5], \"postings on disk incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement `invert_write`, which takes in the td_pairs created with parse_block. We use `InvertedIndexWriter` to write them to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting submission/invert_write.py\n"
     ]
    }
   ],
   "source": [
    "%%tee submission/invert_write.py\n",
    "class BSBIIndex(BSBIIndex):\n",
    "    def invert_write(self, td_pairs, index):\n",
    "        \"\"\"Inverts td_pairs into postings_lists and writes them to the given index\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        td_pairs: List[Tuple[Int, Int]]\n",
    "            List of termID-docID pairs\n",
    "        index: InvertedIndexWriter\n",
    "            Inverted index on disk corresponding to the block       \n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        postings_lists = {}\n",
    "        for td_pair in td_pairs:\n",
    "            if td_pair[0] not in postings_lists:\n",
    "                postings_lists[td_pair[0]] = [td_pair[1]]\n",
    "            else:\n",
    "                if td_pair[1] not in postings_lists[td_pair[0]]:\n",
    "                    postings_lists[td_pair[0]].append(td_pair[1])\n",
    "        keys = list(postings_lists.keys())\n",
    "        keys.sort()\n",
    "        for term in keys:\n",
    "            index.append(term, postings_lists[term])\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this on the toy data, we can read a block and see what the inverted index contains. Write some tests like you saw for the `InvertedIndexWriter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin your code\n",
    "BSBI_instance = BSBIIndex(data_dir=toy_dir, output_dir = 'tmp/', index_name = 'toy')\n",
    "BSBI_instance.parse_block('0')\n",
    "with InvertedIndexWriter('test2', directory='tmp/') as index:\n",
    "    td_pairs = BSBI_instance.parse_block('0')\n",
    "    BSBI_instance.invert_write(td_pairs, index)\n",
    "    index.index_file.seek(0)\n",
    "    assert index.terms == [0,1,2,3,4,5,6], \"terms sequence incorrect\"\n",
    "    assert index.postings_dict[0] == (0, 1, len(UncompressedPostings.encode([0]))), \"postings_dict incorrect\"\n",
    "    assert UncompressedPostings.decode(index.index_file.read()) == [0,0,0,0,1,1,1,1], \"postings on disk incorrect\"\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging\n",
    "\n",
    "> The algorithm simultaneously merges the ten blocks into one large merged index. To do the merging, we open all block files simultaneously, and maintain small read buffers for the ten blocks we are reading and a write buffer for the final merged index we are writing. \n",
    "\n",
    "The iterator paradigm in Python lends itself naturally to our need of maintaining a small read buffer. We can iterate through the file while reading just one postings list at a time from the disk. We subclass `InvertedIndex` into `InvertedIndexIterator` which does the iteration bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting submission/inverted_index_iterator.py\n"
     ]
    }
   ],
   "source": [
    "%%tee submission/inverted_index_iterator.py\n",
    "class InvertedIndexIterator(InvertedIndex):\n",
    "    \"\"\"\"\"\"\n",
    "    def __enter__(self):\n",
    "        \"\"\"Adds an initialization_hook to the __enter__ function of super class\n",
    "        \"\"\"\n",
    "        super().__enter__()\n",
    "        self._initialization_hook()\n",
    "        return self\n",
    "\n",
    "    def _initialization_hook(self):\n",
    "        \"\"\"Use this function to initialize the iterator\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        self.foot = 0\n",
    "        ### End your code\n",
    "\n",
    "    def __iter__(self): \n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        \"\"\"Returns the next (term, postings_list) pair in the index.\n",
    "        \n",
    "        Note: This function should only read a small amount of data from the \n",
    "        index file. In particular, you should not try to maintain the full \n",
    "        index file in memory.\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        if self.foot >= len(self.terms):\n",
    "            return 0\n",
    "        term = self.terms[self.foot]\n",
    "        self.foot = self.foot + 1\n",
    "        start = self.postings_dict[term][0]\n",
    "        nums = self.postings_dict[term][1]\n",
    "        length = self.postings_dict[term][2]\n",
    "        self.index_file.seek(start, 0)\n",
    "        posting_list = self.index_file.read(length)\n",
    "        return self.postings_encoding.decode(posting_list)\n",
    "        ### End your code\n",
    "\n",
    "    def delete_from_disk(self):\n",
    "        \"\"\"Marks the index for deletion upon exit. Useful for temporary indices\n",
    "        \"\"\"\n",
    "        self.delete_upon_exit = True\n",
    "\n",
    "    def __exit__(self, exception_type, exception_value, traceback):\n",
    "        \"\"\"Delete the index file upon exiting the context along with the\n",
    "        functions of the super class __exit__ function\"\"\"\n",
    "        self.index_file.close()\n",
    "        if hasattr(self, 'delete_upon_exit') and self.delete_upon_exit:\n",
    "            os.remove(self.index_file_path)\n",
    "            os.remove(self.metadata_file_path)\n",
    "        else:\n",
    "            with open(self.metadata_file_path, 'wb') as f:\n",
    "                pkl.dump([self.postings_dict, self.terms], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this by first creating an index with the `InvertedIndexWriter` and then iterating through it. Write a few small test cases to see if it works. At the very least, you should write a test which manually constructs a small inverted index, writes them to disk with an `InvertedIndexWriter`, then uses an `InvertedIndexIterator` to iterate over the inverted index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4]\n",
      "[3, 4, 5]\n",
      "[3, 5, 1, 2]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "### Begin your code\n",
    "with InvertedIndexWriter('testIterator', directory='tmp/') as index:\n",
    "    index.append(1, [2, 3, 4])\n",
    "    index.append(2, [3, 4, 5])\n",
    "    index.append(5, [3, 5, 1, 2])\n",
    "    index.index_file.seek(0)\n",
    "    \n",
    "with InvertedIndexIterator('testIterator', directory='tmp/') as iters:\n",
    "    print(iters.__next__())\n",
    "    print(iters.__next__())\n",
    "    print(iters.__next__())\n",
    "    print(iters.__next__())\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> During merging, in each iteration, we select the lowest termID that has not been processed yet using a priority queue or a similar data structure. All postings lists for this termID are read and merged, and the merged list is written back to disk. Each read buffer is refilled from its file when necessary.\n",
    "\n",
    "We'll use the `InvertedIndexIterator` to do the reading part and `InvertedIndexWriter` to write the merged postings list. \n",
    "\n",
    "Note that we've been opening each inverted index using `with` statement, but now we need to do that programmatically. You can see that we've used `contextlib` ([documentation](https://docs.python.org/3/library/contextlib.html#contextlib.ExitStack)) for that in the provided `index` function of `BSBIIndex` base class. Your task is to **write the logic** for merging *opened* `InvertedIndexIterator` objects and writing one postings list at a time into into a single `InvertedIndexWriter` object. \n",
    "\n",
    "Since we know that the postings are sorted, we can appropriately merge them in sorted order in linear time. In fact `heapq` ([documentation](https://docs.python.org/3.0/library/heapq.html)) is a standard python module that provides an implementation of the heap queue algorithm. In particular it contains a `heapq.merge` utility function which merges multiple sorted inputs into a single sorted output and returns an iterator over the sorted values. Not only can this be handy with merging postings lists, but also with merging the inverted indices. \n",
    "\n",
    "To get you started on using the `heapq.merge` function, we've provided a sample usage of the function. The two lists contain animals/birds sorted by their average life span. We want to merge the two lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Giraffe', 28)\n",
      "('Rhinoceros', 40)\n",
      "('Gray Birch', 50)\n",
      "('Indian Elephant', 70)\n",
      "('Black Willow', 70)\n",
      "('Golden Eagle', 80)\n",
      "('Basswood', 100)\n",
      "('Box turtle', 123)\n",
      "('Bald Cypress', 600)\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "animal_lifespans = [('Giraffe', 28), \n",
    "                   ('Rhinoceros', 40), \n",
    "                   ('Indian Elephant', 70), \n",
    "                   ('Golden Eagle', 80), \n",
    "                   ('Box turtle', 123)]\n",
    "\n",
    "tree_lifespans = [('Gray Birch', 50), \n",
    "                  ('Black Willow', 70), \n",
    "                  ('Basswood', 100),\n",
    "                  ('Bald Cypress', 600)]\n",
    "\n",
    "lifespan_lists = [animal_lifespans, tree_lifespans]\n",
    "\n",
    "for merged_item in heapq.merge(*lifespan_lists, key=lambda x: x[1]):\n",
    "    print(merged_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the use of `*` to unpack `lifespan_lists` as arguments and `lambda` function to reprsent the key to perform the soring with. If you are unfamiliar you can check documentation ([unpacking lists](https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists), [lambda expressions](https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions)) or tutorials ([unpacking lists](https://www.geeksforgeeks.org/packing-and-unpacking-arguments-in-python/), [lambda_expressions](https://www.afternerd.com/blog/python-lambdas/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting submission/merge.py\n"
     ]
    }
   ],
   "source": [
    "%%tee submission/merge.py\n",
    "\n",
    "import heapq\n",
    "class BSBIIndex(BSBIIndex):\n",
    "    def merge(self, indices, merged_index):\n",
    "        \"\"\"Merges multiple inverted indices into a single index\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        indices: List[InvertedIndexIterator]\n",
    "            A list of InvertedIndexIterator objects, each representing an\n",
    "            iterable inverted index for a block\n",
    "        merged_index: InvertedIndexWriter\n",
    "            An instance of InvertedIndexWriter object into which each merged \n",
    "            postings list is written out one at a time\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        feet = [0] * len(indices)\n",
    "        parsed = 0\n",
    "        while True:\n",
    "            ret = []\n",
    "            term, selected, feet, parsed = self._take_smallest(indices, feet)\n",
    "            if parsed >= len(indices):\n",
    "                break\n",
    "            postings_lists = self._get_postings_lists(indices, selected)\n",
    "            for merged_item in heapq.merge(*postings_lists):\n",
    "                ret.append(merged_item)\n",
    "            # print(term, ret)\n",
    "            merged_index.append(term, ret)\n",
    "        ### End your code\n",
    "        \n",
    "    def _take_smallest(self, indices, feet):\n",
    "        smallest = None\n",
    "        selected = []\n",
    "        parsed = 0\n",
    "        for i in range(len(feet)):\n",
    "            if feet[i] == -1:\n",
    "                parsed = parsed + 1\n",
    "                continue\n",
    "            if len(indices[i].terms) <= feet[i]:\n",
    "                parsed = parsed + 1\n",
    "                feet[i] = -1\n",
    "            else:\n",
    "                if smallest is None:\n",
    "                    smallest = indices[i].terms[feet[i]]\n",
    "                    selected = [i]\n",
    "                else:\n",
    "                    if smallest > indices[i].terms[feet[i]]:\n",
    "                        smallest = indices[i].terms[feet[i]]\n",
    "                        selected = [i]\n",
    "                    elif smallest == indices[i].terms[feet[i]]:\n",
    "                        selected.append(i)\n",
    "        new_feet = feet\n",
    "        for sel in selected:\n",
    "            new_feet[sel] = new_feet[sel] + 1\n",
    "        return smallest, selected, new_feet, parsed\n",
    "    \n",
    "    def _get_postings_lists(self, indices, selected):\n",
    "        postings_lists = []\n",
    "        for sel in selected:\n",
    "            postings_lists.append(indices[sel].__next__())\n",
    "        return postings_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First make sure it works without errors on toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[(0, 0), (1, 0), (2, 0), (3, 0), (4, 1), (4, 1), (5, 1), (6, 1), (3, 1)]\n",
      "1\n",
      "[(7, 2), (8, 2), (9, 2), (3, 2), (10, 2), (9, 2), (3, 2), (11, 2), (10, 3), (10, 3), (10, 3)]\n",
      "2\n",
      "[(0, 4), (1, 4), (2, 4), (3, 4), (4, 5), (4, 5), (5, 5), (6, 5), (3, 5)]\n"
     ]
    }
   ],
   "source": [
    "BSBI_instance = BSBIIndex(data_dir=toy_dir, output_dir = 'toy_output_dir', )\n",
    "BSBI_instance.index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets index the full corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BSBI_instance = BSBIIndex(data_dir='pa1-data', output_dir = 'output_dir', )\n",
    "BSBI_instance.index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're having errors with just the merging part of it, you can use the following code to debug it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSBI_instance = BSBIIndex(data_dir='pa1-data', output_dir = 'output_dir', )\n",
    "BSBI_instance.intermediate_indices = ['index_'+str(i) for i in range(10)]\n",
    "with InvertedIndexWriter(BSBI_instance.index_name, directory=BSBI_instance.output_dir, postings_encoding=BSBI_instance.postings_encoding) as merged_index:\n",
    "    with contextlib.ExitStack() as stack:\n",
    "        indices = [stack.enter_context(InvertedIndexIterator(index_id, directory=BSBI_instance.output_dir, postings_encoding=BSBI_instance.postings_encoding)) for index_id in BSBI_instance.intermediate_indices]\n",
    "        BSBI_instance.merge(indices, merged_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean conjunctive retrieval\n",
    "\n",
    "We'll be implementing a function `retrieve` to BSBIIndex, which given a query string consisting of space separated tokens, returns a list of documents that contain each of the tokens in the query. However, we do not want to be iterating through the index nor loading the entire index to find the relevant terms. \n",
    "\n",
    "First we'll implement `InvertedIndexMapper` which subclasses `InvertedIndex` to add functionality for retrieving postings corresponding to a particular term by seeking to that location in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting submission/inverted_index_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%tee submission/inverted_index_mapper.py\n",
    "class InvertedIndexMapper(InvertedIndex):\n",
    "    def __getitem__(self, key):\n",
    "        return self._get_postings_list(key)\n",
    "    \n",
    "    def _get_postings_list(self, term):\n",
    "        \"\"\"Gets a postings list (of docIds) for `term`.\n",
    "        \n",
    "        This function should not iterate through the index file.\n",
    "        I.e., it should only have to read the bytes from the index file\n",
    "        corresponding to the postings list for the requested term.\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        if term not in self.postings_dict:\n",
    "            return []\n",
    "        start_position = self.postings_dict[term][0]\n",
    "        byte_length = self.postings_dict[term][2]\n",
    "        self.index_file.seek(start_position)\n",
    "        posting_list_byte = self.index_file.read(byte_length)\n",
    "        posting_list = self.postings_encoding.decode(posting_list_byte)\n",
    "        return posting_list\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a few tests to check your `_get_postings_list` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 3, 181, 252, 253, 261, 262, 267, 270, 271, 272, 273, 276, 281, 282, 290, 291, 463, 464, 477, 479, 481, 482, 484, 488, 489, 491, 614, 620, 621, 652, 765, 957, 1056, 1058, 1075, 1117, 1137, 1250, 1420, 1455, 1563, 1635, 1644, 1748, 1751, 1838, 1847, 1890, 1891, 1895, 1897, 1915, 1931, 1932, 1934, 1942, 2201, 2592, 2594, 2596, 2597, 2598, 2599, 2600, 2601, 2680, 3311, 3329, 3374, 3417, 3430, 3648, 3657, 3660, 3878, 3974, 4000, 4001, 4002, 4003, 4004, 4005, 4006, 4007, 4008, 4009, 4010, 4011, 4012, 4013, 4014, 4015, 4016, 4017, 4018, 4019, 4020, 4021, 4022, 4023, 4024, 4025, 4026, 4027, 4028, 4029, 4030, 4031, 4032, 4033, 4034, 4035, 4036, 4037, 4038, 4039, 4040, 4058, 4059, 4066, 4067, 4071, 4075, 4076, 4077, 4078, 4079, 4080, 4122, 4125, 4140, 4142, 4143, 4211, 4217, 4291, 4296, 4310, 4317, 4351, 4366, 4368, 4369, 4370, 4371, 4372, 4373, 4374, 4375, 4376, 4377, 4378, 4379, 4380, 4381, 4382, 4384, 4386, 4388, 4389, 4390, 4391, 4394, 4395, 4396, 4397, 4398, 4399, 4400, 4401, 4402, 4403, 4404, 4405, 4721, 4771, 4772, 4774, 4789, 4955, 4961, 4965, 4970, 4977, 5023, 5024, 5262, 5307, 5901, 5931, 5934, 6142, 6646, 7118, 7294, 7297, 7306, 7512, 7687, 7688, 7689, 7690, 7696, 7697, 7698, 7699, 7700, 7701, 7702, 7703, 7704, 7705, 7706, 7707, 7708, 7709, 7710, 7711, 7712, 7713, 7714, 7715, 7716, 7717, 7718, 7719, 7720, 7721, 7722, 7723, 7862, 7994, 7997, 8001, 8025, 8176, 8189, 8192, 8222, 8223, 8234, 8298, 8894, 8896, 8909, 9284, 9325, 9564, 9596, 9602, 9619, 9620, 9621, 9622, 9623, 9624, 9625, 9626, 9627, 9628, 9629, 9667, 9683, 9697, 9979, 10037, 10243, 10371, 11544, 11561, 12006, 12090, 12093, 12094, 12095, 12096, 12097, 12098, 12099, 12100, 12101, 12102, 12103, 12104, 12105, 12106, 12107, 12161, 12265, 12282, 12283, 12284, 12285, 12288, 12289, 12290, 12291, 12292, 12293, 12294, 12295, 12296, 12297, 12298, 12299, 12300, 12319, 12321, 12322, 12323, 12324, 12325, 12326, 12327, 12328, 12329, 12330, 12331, 12332, 12333, 12334, 12335, 12336, 12337, 12338, 12339, 12340, 12341, 12342, 12343, 12344, 12345, 12346, 12347, 12348, 12349, 12350, 12351, 12352, 12353, 12355, 12356, 12357, 12358, 12359, 12360, 12361, 12362, 12363, 12364, 12365, 12366, 12367, 12368, 12369, 12370, 12371, 12372, 12373, 12376, 12377, 12378, 12379, 12380, 12381, 12382, 12383, 12384, 12385, 12386, 12387, 12522, 12528, 12683, 12929, 12930, 12933, 13239, 13255, 13308, 14067, 14211, 14320, 14385, 14432, 15135, 15212, 15322, 15362, 15364, 15395, 15459, 15469, 15494, 15505, 15524, 15766, 15781, 15808, 15833, 15904, 15908, 16234, 16350, 16535, 16541, 16542, 16579, 16587, 16599, 16648, 16649, 16874, 16903, 17403, 17506, 18268, 18521, 18529, 18800, 19124, 19176, 19198, 19199, 19200, 19205, 19206, 19207, 19208, 19210, 19254, 19280, 19485, 19857, 19860, 19861, 19865, 19867, 19868, 19869, 19870, 19871, 19872, 19873, 19874, 19875, 19876, 19879, 19880, 19881, 19882, 19883, 19960, 20222, 20266, 21458, 21652, 21794, 22012, 23411, 23446, 23717, 23732, 24190, 24438, 24704, 24712, 24979, 24985, 24986, 25149, 25653, 25654, 25663, 25779, 25793, 26035, 26167, 26407, 26801, 26861, 29770, 29789, 29795, 30034, 30041, 30042, 30227, 30256, 30257, 30259, 30260, 30270, 30292, 30312, 30313, 30508, 31171, 31215, 31221, 31330, 31355, 31372, 31415, 31438, 31615, 31641, 31651, 31951, 31952, 31953, 31964, 31972, 31992, 31994, 31996, 32004, 32118, 32131, 32139, 32144, 32149, 32164, 32184, 32196, 32220, 32221, 32222, 32224, 32225, 32228, 32239, 32265, 32444, 32575, 32778, 32977, 33336, 33639, 33798, 33805, 33818, 33827, 34124, 34125, 34126, 34187, 34206, 34231, 34237, 34261, 34269, 34602, 34612, 34613, 34614, 34615, 34616, 34617, 34619, 34620, 34621, 34623, 34624, 34625, 34626, 34627, 34628, 34629, 34630, 34631, 34632, 34633, 34634, 34635, 34636, 34637, 34638, 34639, 34640, 34641, 34642, 34643, 34644, 34645, 34646, 34647, 34648, 34649, 34650, 34651, 34652, 34653, 34654, 34655, 34656, 34657, 34658, 34659, 34660, 34661, 34662, 34663, 34664, 34665, 34666, 34667, 34668, 34669, 34670, 34671, 34672, 34673, 34674, 34675, 34676, 34677, 34678, 34679, 34680, 34681, 34682, 34683, 34684, 34685, 34686, 34687, 34688, 34689, 34690, 34691, 34692, 34693, 34694, 34695, 34696, 34697, 34698, 34699, 34700, 34701, 34702, 34703, 34704, 34705, 34706, 34707, 34708, 34709, 34710, 34711, 34712, 34713, 34714, 34715, 34716, 34717, 34718, 34719, 34720, 34721, 34722, 34723, 34724, 34725, 34726, 34727, 34728, 34729, 34730, 34731, 34732, 34733, 34734, 34735, 34736, 34737, 34738, 34739, 34740, 34741, 34742, 34743, 34744, 34745, 34746, 34747, 34748, 34749, 34750, 34751, 34752, 34753, 34754, 34755, 34756, 34757, 34758, 34759, 34760, 34761, 34762, 34763, 34764, 34765, 34766, 34767, 34768, 34769, 34770, 34771, 34772, 34773, 34774, 34775, 34776, 34777, 34778, 34779, 34780, 34781, 34782, 34783, 34784, 34785, 34786, 34787, 34788, 34789, 34790, 34791, 34792, 34793, 34794, 34795, 34796, 34797, 34798, 34799, 34800, 34801, 34802, 34803, 34804, 34805, 34806, 34807, 34808, 34809, 34810, 34811, 34812, 34813, 34814, 34815, 34816, 34817, 34818, 34819, 34820, 34822, 34823, 34835, 34855, 34859, 34860, 34865, 34919, 34920, 34933, 34941, 34942, 34944, 34946, 34947, 34948, 34956, 34961, 34983, 34992, 34999, 35004, 35005, 35006, 35007, 35008, 35009, 35010, 35011, 35013, 35014, 35015, 35016, 35017, 35018, 35019, 35020, 35021, 35022, 35023, 35024, 35025, 35026, 35027, 35028, 35029, 35030, 35031, 35032, 35033, 35034, 35035, 35036, 35037, 35038, 35039, 35040, 35041, 35042, 35043, 35044, 35045, 35046, 35047, 35048, 35059, 35060, 35061, 35062, 35063, 35064, 35065, 35066, 35067, 35068, 35069, 35070, 35071, 35072, 35073, 35074, 35075, 35076, 35077, 35078, 35079, 35080, 35081, 35082, 35083, 35084, 35085, 35086, 35087, 35088, 35089, 35090, 35091, 35092, 35093, 35094, 35095, 35096, 35097, 35098, 35099, 35100, 35101, 35102, 35103, 35104, 35105, 35106, 35107, 35108, 35109, 35110, 35111, 35112, 35113, 35115, 35116, 35117, 35118, 35119, 35120, 35121, 35122, 35123, 35124, 35125, 35126, 35128, 35129, 35130, 35131, 35142, 35144, 35145, 35146, 35147, 35148, 35149, 35150, 35151, 35152, 35153, 35154, 35155, 35156, 35157, 35158, 35159, 35160, 35161, 35162, 35163, 35164, 35165, 35166, 35167, 35267, 35268, 35295, 35297, 35305, 35371, 35394, 35397, 35419, 35422, 35429, 35444, 35485, 35492, 35495, 35498, 35541, 35543, 35581, 35591, 35592, 35658, 35681, 35701, 35713, 35909, 36028, 36103, 36104, 36105, 36106, 36107, 36108, 36109, 36110, 36111, 36112, 36113, 36114, 36115, 36116, 36117, 36118, 36119, 36120, 36121, 36122, 36123, 36124, 36125, 36126, 36127, 36128, 36129, 36130, 36131, 36132, 36133, 36134, 36135, 36136, 36137, 36138, 36139, 36140, 36141, 36142, 36143, 36144, 36145, 36146, 36147, 36148, 36149, 36150, 36151, 36152, 36153, 36154, 36155, 36156, 36157, 36158, 36159, 36160, 36161, 36162, 36163, 36164, 36165, 36166, 36167, 36168, 36169, 36170, 36171, 36172, 36173, 36174, 36175, 36176, 36177, 36178, 36179, 36180, 36181, 36182, 36183, 36184, 36185, 36186, 36187, 36188, 36189, 36190, 36191, 36192, 36193, 36194, 36195, 36196, 36197, 36198, 36199, 36200, 36201, 36202, 36204, 36205, 36206, 36209, 36210, 36214, 36215, 36232, 36234, 36277, 36287, 36444, 36445, 36447, 36450, 36452, 36456, 36457, 36460, 36461, 36462, 36464, 36465, 36511, 36555, 36556, 36557, 36558, 36559, 36586, 36587, 36588, 36590, 36591, 36601, 36602, 36603, 36604, 36605, 36606, 36607, 36608, 36609, 36616, 36617, 36618, 36619, 36622, 36623, 36624, 36625, 36626, 36643, 36645, 36646, 36647, 36648, 36649, 36680, 36681, 36682, 36729, 36856, 36857, 36860, 36862, 36863, 36864, 36867, 36868, 36871, 36873, 36874, 36875, 36876, 36877, 36878, 36899, 36901, 36904, 36905, 36906, 36907, 36916, 36917, 36918, 36919, 36921, 36922, 36928, 36929, 36932, 37045, 37074, 37136, 37141, 37163, 37189, 37405, 37695, 37735, 37743, 37762, 38019, 38036, 38154, 38192, 38195, 38242, 38336, 38352, 38506, 38625, 38654, 38655, 38671, 38685, 38694, 38696, 38803, 38858, 38859, 38860, 38899, 38911, 38991, 38992, 39005, 39006, 39009, 39032, 39033, 39037, 39045, 39049, 39091, 39100, 39103, 39105, 39237, 39238, 39362, 40749, 40794, 40954, 40990, 41048, 41049, 41050, 41238, 41295, 41391, 41392, 41423, 41495, 41496, 41500, 41502, 41547, 41587, 41630, 41642, 41666, 41679, 41707, 41711, 41916, 42439, 42557, 42597, 42608, 42851, 42868, 42884, 43038, 43070, 43072, 43230, 43290, 43336, 43337, 43342, 43353, 43358, 43402, 43452, 43474, 43627, 43652, 43842, 43843, 43844, 43845, 43846, 43847, 43848, 43849, 43850, 43852, 43853, 43854, 44555, 44558, 44711, 44797, 44958, 44990, 45002, 45003, 45005, 45006, 45007, 45027, 45036, 45038, 45044, 45049, 45051, 45052, 45053, 45059, 45064, 45066, 45075, 45084, 45088, 45127, 45227, 45312, 45356, 45357, 45358, 45383, 45536, 45537, 45538, 45539, 45540, 45541, 45655, 45763, 46008, 46023, 46120, 46131, 46146, 46288, 46307, 46318, 46336, 46337, 46339, 46374, 46916, 47257, 47382, 47402, 47404, 47406, 47407, 47412, 47420, 47477, 47481, 47483, 47529, 47566, 47603, 47731, 47741, 47768, 47917, 47963, 48158, 48465, 48559, 48624, 48759, 49017, 49022, 49023, 49460, 49463, 49464, 49642, 49783, 50169, 50223, 50247, 50479, 50567, 51090, 51091, 51204, 51228, 51232, 51241, 51279, 51282, 51303, 51315, 51744, 51746, 51747, 51748, 51749, 51926, 51927, 51928, 51929, 51984, 52047, 52290, 52291, 52379, 52451, 52452, 52453, 52454, 52455, 52456, 52457, 52458, 52491, 52492, 52493, 52494, 52495, 52496, 52497, 52498, 52714, 53222, 53379, 53461, 53966, 53973, 54308, 54990, 55039, 55352, 55385, 55466, 55471, 55892, 55960, 55978, 56201, 56202, 56203, 56204, 56205, 56206, 56209, 56210, 56211, 56212, 56213, 56214, 56215, 56216, 56217, 56218, 56219, 56220, 56221, 56222, 56223, 56224, 56225, 56226, 56227, 56228, 56229, 56230, 56231, 56232, 56233, 56234, 56235, 56236, 56237, 56238, 56239, 56240, 56241, 56242, 56243, 56244, 56245, 56246, 56247, 56248, 56249, 56250, 56251, 56252, 56253, 56254, 56255, 56256, 56257, 56258, 56259, 56260, 56261, 56262, 56263, 56264, 56265, 56266, 56267, 56268, 56269, 56270, 56271, 56272, 56273, 56274, 56275, 56276, 56277, 56278, 56279, 56280, 56575, 56606, 56714, 57634, 57780, 57869, 57904, 57906, 58003, 58027, 58033, 58047, 58091, 58092, 58095, 58334, 58427, 58439, 58440, 58441, 58442, 58444, 58445, 58457, 58462, 58464, 58465, 58468, 58469, 58472, 58473, 58474, 58475, 58476, 58478, 58479, 58480, 58482, 58483, 58484, 58485, 58487, 58490, 58492, 58494, 58502, 58503, 58504, 58505, 58506, 58507, 58508, 58509, 58510, 58511, 58512, 58514, 58515, 58516, 58517, 58518, 58519, 58520, 58522, 58523, 58539, 58540, 58560, 58562, 58566, 58707, 58709, 58714, 58994, 59000, 59055, 59073, 59104, 59120, 59223, 59230, 59231, 59241, 59247, 59271, 59272, 59294, 59295, 59435, 59554, 59682, 59685, 59703, 59714, 59921, 59922, 59923, 59924, 59925, 59926, 59927, 59928, 59929, 59930, 59931, 59933, 59934, 59935, 59936, 59937, 59938, 59947, 59948, 59953, 60027, 60096, 60149, 60150, 60151, 60172, 60175, 60176, 60177, 60335, 60376, 60410, 60458, 60813, 60931, 61144, 61291, 61397, 61560, 61603, 61624, 61625, 61734, 61937, 61938, 61949, 61950, 61951, 61952, 61953, 61954, 61955, 61956, 61957, 61958, 61959, 61960, 61961, 61962, 61963, 61964, 61965, 61966, 61967, 61968, 61969, 61970, 61971, 61972, 61973, 61974, 61975, 61976, 61977, 61978, 61979, 61980, 61981, 61982, 61983, 61984, 61985, 61986, 61987, 61988, 61989, 61990, 61991, 61992, 61993, 61994, 61995, 61996, 61997, 61998, 61999, 62052, 62208, 62211, 62240, 62242, 62244, 62323, 62421, 62873, 62876, 62892, 62898, 63094, 63096, 63108, 63130, 63131, 63145, 63146, 63149, 63150, 63161, 63193, 63207, 63208, 63209, 63210, 63211, 63212, 63213, 63214, 63215, 63216, 63260, 63277, 63439, 63723, 63728, 63789, 63790, 64250, 64262, 64267, 64273, 64274, 64280, 64285, 64291, 64292, 64308, 64309, 64311, 64319, 64327, 64340, 64341, 64343, 64344, 64346, 64350, 64353, 64367, 64377, 64379, 64381, 64399, 64400, 64412, 64415, 64423, 64440, 64469, 64477, 64490, 64504, 64643, 64681, 64734, 65112, 65149, 65161, 65356, 65396, 65490, 65496, 65540, 65561, 65563, 65607, 65913, 65982, 65983, 66004, 66009, 66010, 66017, 66049, 66194, 66228, 66272, 66517, 66518, 66708, 66710, 66877, 67045, 67175, 67181, 67186, 67234, 67337, 67355, 67380, 67386, 67588, 67608, 67781, 67788, 67823, 67888, 67897, 67998, 68009, 68039, 68066, 68289, 68308, 68389, 68394, 68401, 68408, 68409, 68411, 68412, 68413, 68414, 68469, 68592, 68612, 68614, 68616, 68618, 68619, 68620, 68626, 68628, 68629, 68630, 68631, 68635, 68642, 68643, 68644, 68645, 68646, 68647, 68648, 68655, 68656, 68662, 68663, 68664, 68665, 68666, 68671, 68673, 68674, 68675, 68676, 68677, 68680, 68681, 68689, 68691, 68692, 68693, 68694, 68695, 68698, 68699, 68700, 68701, 68702, 68703, 68704, 68708, 68719, 68720, 68723, 68725, 68727, 68728, 68731, 68733, 68736, 68738, 68739, 68740, 68742, 68755, 68757, 68758, 68759, 68760, 68763, 68764, 68772, 68776, 68797, 68800, 68802, 68803, 68813, 68814, 68817, 68823, 68833, 68841, 68843, 68845, 68849, 68854, 68859, 68860, 68864, 68865, 68868, 68872, 68875, 68878, 68879, 68884, 68886, 68887, 68888, 68894, 68895, 68898, 68899, 68900, 68903, 68904, 68905, 68906, 68908, 68909, 68911, 68912, 68917, 68933, 68957, 68959, 68960, 68972, 68993, 68995, 68999, 69012, 69015, 69023, 69085, 69088, 69092, 69096, 69104, 69124, 69131, 69135, 69214, 69215, 69279, 69305, 69308, 69313, 69314, 69316, 69317, 69320, 69334, 69346, 69354, 69368, 69375, 69376, 69377, 69378, 69379, 69380, 69381, 69382, 69394, 69399, 69425, 69428, 69435, 69436, 69546, 69883, 69943, 69964, 70072, 70126, 70184, 70186, 70190, 70206, 70207, 70219, 70344, 70676, 70690, 70718, 70720, 70803, 70820, 70890, 70920, 70928, 70933, 70935, 70942, 71021, 71023, 71028, 71034, 71036, 71037, 71038, 71039, 71040, 71041, 71042, 71043, 71044, 71045, 71046, 71047, 71048, 71049, 71050, 71051, 71052, 71053, 71054, 71055, 71056, 71057, 71058, 71059, 71060, 71061, 71062, 71081, 71089, 71128, 71157, 71158, 71163, 71164, 71165, 71166, 71167, 71175, 71219, 71240, 71241, 71242, 71243, 71246, 71249, 71265, 71282, 71330, 71343, 71363, 71368, 71371, 71470, 71516, 71517, 71520, 71634, 71636, 71639, 71645, 71704, 71732, 71779, 71841, 71862, 72016, 72026, 72027, 72028, 72045, 72046, 72048, 72049, 72050, 72054, 72058, 72059, 72060, 72062, 72063, 72148, 72243, 72469, 72472, 72576, 72827, 72836, 73079, 73421, 73616, 73729, 75643, 75656, 75830, 75853, 76080, 76081, 76102, 76155, 76156, 76185, 76287, 76291, 76293, 76295, 76686, 76787, 77068, 77214, 77281, 77296, 77365, 77378, 77381, 77444, 77450, 77493, 77506, 77511, 77513, 77514, 77524, 77525, 77538, 77545, 77553, 77555, 77556, 77558, 77574, 77580, 77586, 77592, 77593, 77595, 77610, 77619, 80645, 80754, 80757, 81139, 81212, 81460, 81729, 81882, 81998, 82074, 82082, 82085, 82087, 82090, 82119, 82123, 82132, 82184, 82404, 82963, 82964, 82970, 83149, 83483, 83553, 84385, 84397, 84399, 84658, 84723, 84726, 84728, 84730, 84732, 84734, 84738, 84758, 84773, 84790, 84808, 84819, 84829, 84866, 84974, 84980, 85069, 85072, 85086, 86190, 86795, 87165, 87224, 87228, 87232, 87258, 87274, 87353, 87363, 87364, 87368, 87374, 87393, 87394, 87395, 87396, 87397, 87400, 87414, 87419, 87425, 87426, 87428, 87430, 87432, 87443, 87451, 87454, 87458, 87459, 87460, 87461, 87465, 87466, 87467, 87468, 87470, 87471, 87472, 87473, 87476, 87477, 87478, 87482, 87497, 87498, 87500, 87514, 87526, 87535, 87538, 87581, 87635, 87649, 87861, 87872, 88355, 88909, 89040, 89054, 89072, 89073, 89075, 89076, 89112, 89224, 89443, 89482, 89495, 89509, 89534, 89615, 89625, 89683, 89688, 89725, 89729, 89910, 89911, 89912, 90007, 90205, 90207, 90208, 90210, 90211, 90463, 90515, 90806, 90957, 90960, 91077, 91762, 91790, 91875, 91936, 92072, 92073, 92074, 92075, 92076, 92077, 92078, 92079, 92080, 92081, 92088, 92089, 92090, 92091, 92092, 92093, 92100, 92101, 92102, 92103, 92104, 92105, 92106, 92107, 92108, 92109, 92110, 92111, 92112, 92113, 92114, 92115, 92116, 92118, 92121, 92126, 92127, 92128, 92129, 92130, 92131, 92132, 92133, 92134, 92135, 92136, 92137, 92138, 92139, 92200, 92328, 92329, 92330, 92331, 92335, 92336, 92365, 92367, 92416, 92417, 92725, 92735, 92736, 92785, 92812, 92839, 92887, 92890, 92897, 92932, 92957, 92961, 92962, 92963, 92964, 92965, 92972, 92973, 92974, 92975, 92976, 92977, 92978, 92981, 92992, 92993, 92996, 92997, 92998, 92999, 93293, 93298, 93359, 93382, 93434, 93516, 93581, 93582, 93586, 93587, 93600, 93603, 93616, 93620, 93621, 94207, 94278, 94279, 94280, 94281, 94282, 94283, 94284, 94285, 94286, 94409, 94410, 94411, 94412, 94658, 94800, 95290, 95400, 95433, 95436, 95437, 95440, 95452, 95453, 95457, 95459, 95644, 95645, 95950, 95983, 96362, 96392, 96399, 96402, 96404, 96413, 96437, 96668, 96679, 96680, 96681, 96682, 96683, 96685, 96695, 96697, 96698, 96708, 96966, 97187, 97276, 97306, 97568, 97729, 97757, 98003, 98040, 98075, 98083, 98128, 98129, 98415, 98428, 98429, 98431, 98484, 98485, 98488, 98535, 98561, 98570, 98604, 98661, 98672, 98674, 98708, 98709, 98770, 98819, 98821, 98822, 98823, 98824, 98825, 98826, 98827, 98828, 98829, 98830, 98831, 98832, 98833, 98834, 98835, 98836, 98837, 98838, 98839, 98840, 98841, 98842, 98843, 98844, 98859, 98917, 98971, 98977]\n"
     ]
    }
   ],
   "source": [
    "### Begin your code\n",
    "with InvertedIndexMapper('BSBI', directory='output_dir/') as index:\n",
    "    print(index[140])\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can obtain postings lists corresponding to the query terms, we want to intersect them. However, à la merging, we can use the sorted-ness of these lists to intersect them very efficiently. Here we'll implement `sorted_intersect` which takes two sorted lists and return a sorted intersection of the elements.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting submission/sorted_intersect.py\n"
     ]
    }
   ],
   "source": [
    "%%tee submission/sorted_intersect.py\n",
    "def sorted_intersect(list1, list2):\n",
    "    \"\"\"Intersects two (ascending) sorted lists and returns the sorted result\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    list1: List[Comparable]\n",
    "    list2: List[Comparable]\n",
    "        Sorted lists to be intersected\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List[Comparable]\n",
    "        Sorted intersection        \n",
    "    \"\"\"\n",
    "    ### Begin your code\n",
    "    result = []\n",
    "    for elem in list1:\n",
    "        if elem in list2:\n",
    "            result.append(elem)\n",
    "    return result\n",
    "    ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a few simple test cases to make sure it works correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 10]\n"
     ]
    }
   ],
   "source": [
    "### Begin your code\n",
    "list1 = [1,3,5,8,10]\n",
    "list2 = [2,4,8,10,15]\n",
    "print(sorted_intersect(list1, list2))\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to write the `retrieve` function using `InvertedIndexMapper` and `sorted_intersect`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting submission/retrieve.py\n"
     ]
    }
   ],
   "source": [
    "%%tee submission/retrieve.py\n",
    "class BSBIIndex(BSBIIndex):\n",
    "    def retrieve(self, query):\n",
    "        \"\"\"Retrieves the documents corresponding to the conjunctive query\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        query: str\n",
    "            Space separated list of query tokens\n",
    "            \n",
    "        Result\n",
    "        ------\n",
    "        List[str]\n",
    "            Sorted list of documents which contains each of the query tokens. \n",
    "            Should be empty if no documents are found.\n",
    "        \n",
    "        Should NOT throw errors for terms not in corpus\n",
    "        \"\"\"\n",
    "        if len(self.term_id_map) == 0 or len(self.doc_id_map) == 0:\n",
    "            self.load()\n",
    "\n",
    "        ### Begin your code\n",
    "        import re\n",
    "        with InvertedIndexMapper('BSBI', directory='output_dir/') as mapper:\n",
    "            query = re.sub('[,.!?]', '', query)\n",
    "            query = re.sub('\\s+', ' ', query)\n",
    "            tokens = query.strip().split(' ')\n",
    "            result = None\n",
    "            for token in tokens:\n",
    "                token = self.term_id_map[token]\n",
    "                if result is None:\n",
    "                    result = mapper[token]\n",
    "                else:\n",
    "                    result = sorted_intersect(result, mapper[token])\n",
    "            for i in range(len(result)):\n",
    "                result[i] = self.doc_id_map[result[i]]\n",
    "            return result\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test if our index works on the real corpus with a simple query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pa1-data\\\\1\\\\cs276.stanford.edu_',\n",
       " 'pa1-data\\\\1\\\\cs276a.stanford.edu_',\n",
       " 'pa1-data\\\\3\\\\infolab.stanford.edu_TR_CS-TN-95-21.html',\n",
       " 'pa1-data\\\\4\\\\nlp.stanford.edu_IR-book_',\n",
       " 'pa1-data\\\\4\\\\nlp.stanford.edu_IR-book_html_htmledition_an-appraisal-of-probabilistic-models-1.html',\n",
       " 'pa1-data\\\\4\\\\nlp.stanford.edu_IR-book_html_htmledition_bayesian-network-approaches-to-ir-1.html',\n",
       " 'pa1-data\\\\4\\\\nlp.stanford.edu_IR-book_html_htmledition_boolean-retrieval-2.html',\n",
       " 'pa1-data\\\\4\\\\nlp.stanford.edu_IR-book_html_htmledition_computing-scores-in-a-complete-search-system-1.html',\n",
       " 'pa1-data\\\\4\\\\nlp.stanford.edu_IR-book_html_htmledition_dictionaries-and-tolerant-retrieval-1.html',\n",
       " 'pa1-data\\\\4\\\\nlp.stanford.edu_IR-book_html_htmledition_efficient-scoring-and-ranking-1.html',\n",
       " 'pa1-data\\\\4\\\\nlp.stanford.edu_IR-book_html_htmledition_inexact-top-k-document-retrieval-1.html',\n",
       " 'pa1-data\\\\4\\\\nlp.stanford.edu_IR-book_html_htmledition_probabilistic-information-retrieval-1.html',\n",
       " 'pa1-data\\\\4\\\\nlp.stanford.edu_IR-book_html_htmledition_putting-it-all-together-1.html',\n",
       " 'pa1-data\\\\4\\\\nlp.stanford.edu_IR-book_html_htmledition_references-and-further-reading-1.html',\n",
       " 'pa1-data\\\\4\\\\nlp.stanford.edu_IR-book_html_htmledition_the-search-user-experience-1.html',\n",
       " 'pa1-data\\\\4\\\\nlp.stanford.edu_IR-book_html_htmledition_vector-space-scoring-and-query-operator-interaction-1.html',\n",
       " 'pa1-data\\\\4\\\\nlp.stanford.edu_IR-book_html_htmledition_weighted-zone-scoring-1.html',\n",
       " 'pa1-data\\\\4\\\\nlp.stanford.edu_IR-book_html_htmledition_wildcard-queries-2.html',\n",
       " 'pa1-data\\\\4\\\\nlp.stanford.edu_IR-book_newslides.html',\n",
       " 'pa1-data\\\\6\\\\www-nlp.stanford.edu_IR-book_essir2011_',\n",
       " 'pa1-data\\\\8\\\\www.stanford.edu_class_cs124_']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BSBI_instance = BSBIIndex(data_dir='pa1-data', output_dir = 'output_dir', )\n",
    "BSBI_instance.retrieve('boolean retrieval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also test if one of these pages truly contains the query terms by reading a file as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs276 information retrieval and web search cs 276 ling 286 information retrieval and web search spring 2011 pandu nayak and prabhakar raghavan lecture 3 units tu th 4 15 5 30 gates b03 tas sonali aggarwal ivan cui valentin spitkovsky and sandeep sripada staff e mail cs276 spr1011 staff lists stanford edu lectures are also available online and on television through scpd sitn course description basic and advanced techniques for text based information systems efficient text indexing boolean and vector space retrieval models evaluation and interface issues web search including crawling link based algorithms and web metadata text web clustering classification text mining syllabus additional information staff contact information piazzza we strongly recommend students to post questions to the course page on www piazzza com instead of sending emails this forum enables students to discuss the questions they encounter in lectures or assignments here is a quick introduction video email if you have a question not appropriate for the piazzza forum eg one that is only relevant to your situation or one that reveals part of your solution to a homework question please email the staff mailing list at cs276 spr1011 staff lists stanford edu professor pandu nayak office none on campus office hours by appointment email nayak cs stanford edu professor prabhakar raghavan office none on campus office hours by appointment email pragh cs stanford edu ta sonali aggarwal office location b26 a ph 650 723 6319 office hours wednesday 4 30pm 6 30pm email sonali9 stanford edu ta ivan cui office location b26 b ph 650 736 1817 office hours monday 2 15pm 4 15pm email ivancui stanford edu ta valentin spitkovsky office location gates 228 second floor office hours thursday 2 15pm 4 15pm email vals stanford edu ta sandeep sripada office location b26 a ph 650 723 6319 office hours tuesday 5 30pm 7 30pm remember to carry your stanford id to open the basement doors email sss cs stanford edu announcements if you are not registered in the course but wish to receive course announcements you may subscribe to the guest mailing list cs276 spr1011 guests lists stanford edu textbook introduction to information retrieval by c manning p raghavan and h schutze cambridge university press available from the stanford bookstore or other fine retailers you can also download and print chapters at the book website prerequisites cs 103b and cs 107 and any one of cs 121 cs 145 or cs 161 or equivalent background programming experience will be necessary for the two practical exercises related courses introduction to computational advertising http www stanford edu class msande239\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"pa1-data/1/cs276.stanford.edu_\", 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test on the dev queries to see if it works alright. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results match for query: we are\n",
      "Results match for query: stanford class\n",
      "Results match for query: stanford students\n",
      "Results match for query: very cool\n",
      "Results match for query: the\n",
      "Results match for query: a\n",
      "Results match for query: the the\n",
      "Results match for query: stanford computer science\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir('dev_output')\n",
    "except:\n",
    "    pass\n",
    "for i in range(1, 9):\n",
    "    with open('dev_queries/query.' + str(i)) as q:\n",
    "        query = q.read()\n",
    "        my_results = [os.path.normpath(path) for path in BSBI_instance.retrieve(query)]\n",
    "        with open('dev_output/' + str(i) + '.out', 'wb') as o:\n",
    "            # reference_results = [os.path.normpath(x.strip()) for x in o.readlines()]\n",
    "            # assert my_results == reference_results, \"Results DO NOT match for query: \"+query.strip()\n",
    "            pkl.dump(my_results, o)\n",
    "        print(\"Results match for query:\", query.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If an assert is failing you can compare the difference in outputs as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(my_results) - set(reference_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(reference_results) - set(my_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you write your own queries test for all kinds of corner cases like terms which have never occurred in the corpus, or terms that are very frequent and might slow down the merge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建压缩索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you should build a compressed index using gap encoding with variable byte encoding for each gap.\n",
    "\n",
    "Here are some things about python you'll find useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 % 2 =  0\n",
      "10 % 3 =  1\n",
      "10 / 3 =  3.3333333333333335\n",
      "10 // 3 =  3\n"
     ]
    }
   ],
   "source": [
    "# Remainder (modulo) operator %\n",
    "\n",
    "print(\"10 % 2 = \", 10 % 2)\n",
    "print(\"10 % 3 = \", 10 % 3)\n",
    "\n",
    "# Integer division in Python 3 is done by using two slash signs\n",
    "\n",
    "print(\"10 / 3 = \", 10 / 3)\n",
    "print(\"10 // 3 = \", 10 // 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the following `CompressedPostings` class which we shall use as a drop-in replacement for `UncompressedPostings`. You should refer to [Chapter 5](https://nlp.stanford.edu/IR-book/pdf/05comp.pdf) of the textbook for understanding how gap encoding with variable byte encoding for each gap is done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting submission/compressed_postings.py\n"
     ]
    }
   ],
   "source": [
    "%%tee submission/compressed_postings.py\n",
    "class CompressedPostings:\n",
    "    #If you need any extra helper methods you can add them here \n",
    "    ### Begin your code\n",
    "    @staticmethod\n",
    "    def prepend(bts, number):\n",
    "        number = number % 128\n",
    "        number = number.to_bytes(length=1, byteorder='little')\n",
    "        if bts is None:\n",
    "            bts = number\n",
    "        else:\n",
    "            bts = number + bts\n",
    "        return bts\n",
    "    \n",
    "    @staticmethod\n",
    "    def encode_number(number):\n",
    "        bts = None\n",
    "        while True:\n",
    "            bts = CompressedPostings.prepend(bts, number)\n",
    "            if number < 128:\n",
    "                break\n",
    "            number = number // 128\n",
    "        bt_modified = (bts[len(bts)-1] + 128).to_bytes(length=1, byteorder='little')\n",
    "        bts = bts[0:len(bts)-1] + bt_modified\n",
    "        return bts\n",
    "    ### End your code\n",
    "    \n",
    "    @staticmethod\n",
    "    def encode(postings_list):\n",
    "        \"\"\"Encodes `postings_list` using gap encoding with variable byte \n",
    "        encoding for each gap\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        postings_list: List[int]\n",
    "            The postings list to be encoded\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        bytes: \n",
    "            Bytes reprsentation of the compressed postings list \n",
    "            (as produced by `array.tobytes` function)\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        bytestream = None\n",
    "        for number in postings_list:\n",
    "            if bytestream is None:\n",
    "                bytestream = CompressedPostings.encode_number(number)\n",
    "            else:\n",
    "                bytestream += CompressedPostings.encode_number(number)\n",
    "        return bytestream\n",
    "        ### End your code\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def decode(encoded_postings_list):\n",
    "        \"\"\"Decodes a byte representation of compressed postings list\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        encoded_postings_list: bytes\n",
    "            Bytes representation as produced by `CompressedPostings.encode` \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        List[int]\n",
    "            Decoded postings list (each posting is a docIds)\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        numbers = []\n",
    "        n = 0\n",
    "        for i in range(len(encoded_postings_list)):\n",
    "            temp = encoded_postings_list[i]\n",
    "            if temp < 128:\n",
    "                n = 128 * n + temp\n",
    "            else:\n",
    "                n = 128 * n + (temp - 128)\n",
    "                numbers.append(n)\n",
    "                n = 0\n",
    "        return numbers\n",
    "        ### End your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, write some test cases for any helper methods that you might have implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x81\\x83\\x86\\x88\\x8a\\x86\\x82\\xce\\xad\\x9a_\\xa8'\n",
      "[1, 3, 6, 8, 10, 6, 2, 78, 45, 26, 12200]\n"
     ]
    }
   ],
   "source": [
    "### Begin your code\n",
    "postings_list = [1,3,6,8,10,6,2,78,45,26,12200]\n",
    "a = CompressedPostings.encode(postings_list)\n",
    "print(a)\n",
    "b = CompressedPostings.decode(a)\n",
    "print(b)\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've provided a simple helper function for you to test whether an encoded postings list is decode correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_encode_decode(l):\n",
    "    e = CompressedPostings.encode(l)\n",
    "    d = CompressedPostings.decode(e)\n",
    "    assert d == l\n",
    "    print(l, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a few test cases to make sure the postings list compression and decompression is being done correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x81\\x83\\x86\\x88\\x8a\\x86\\x82\\xce\\xad\\x9a\\xfa'\n",
      "[1, 3, 6, 8, 10, 6, 2, 78, 45, 26, 122]\n"
     ]
    }
   ],
   "source": [
    "### Begin your code\n",
    "postings_list = [1,3,6,8,10,6,2,78,45,26,122]\n",
    "a = CompressedPostings.encode(postings_list)\n",
    "print(a)\n",
    "b = CompressedPostings.decode(a)\n",
    "print(b)\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us create a new output directory and use the `CompressedPostings` to create a compressed index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    os.mkdir('output_dir_compressed')\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BSBI_instance_compressed = BSBIIndex(data_dir='pa1-data', output_dir = 'output_dir_compressed', postings_encoding=CompressedPostings)\n",
    "BSBI_instance_compressed.index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pa1-data\\\\1\\\\cs276.stanford.edu_',\n",
       " 'pa1-data\\\\1\\\\cs276a.stanford.edu_',\n",
       " 'pa1-data\\\\3\\\\infolab.stanford.edu_TR_CS-TN-95-21.html',\n",
       " 'pa1-data\\\\4\\\\nlp.stanford.edu_IR-book_',\n",
       " 'pa1-data\\\\4\\\\nlp.stanford.edu_IR-book_html_htmledition_an-appraisal-of-probabilistic-models-1.html',\n",
       " 'pa1-data\\\\4\\\\nlp.stanford.edu_IR-book_html_htmledition_bayesian-network-approaches-to-ir-1.html',\n",
       " 'pa1-data\\\\4\\\\nlp.stanford.edu_IR-book_html_htmledition_boolean-retrieval-2.html',\n",
       " 'pa1-data\\\\4\\\\nlp.stanford.edu_IR-book_html_htmledition_computing-scores-in-a-complete-search-system-1.html',\n",
       " 'pa1-data\\\\4\\\\nlp.stanford.edu_IR-book_html_htmledition_dictionaries-and-tolerant-retrieval-1.html',\n",
       " 'pa1-data\\\\4\\\\nlp.stanford.edu_IR-book_html_htmledition_efficient-scoring-and-ranking-1.html',\n",
       " 'pa1-data\\\\4\\\\nlp.stanford.edu_IR-book_html_htmledition_inexact-top-k-document-retrieval-1.html',\n",
       " 'pa1-data\\\\4\\\\nlp.stanford.edu_IR-book_html_htmledition_probabilistic-information-retrieval-1.html',\n",
       " 'pa1-data\\\\4\\\\nlp.stanford.edu_IR-book_html_htmledition_putting-it-all-together-1.html',\n",
       " 'pa1-data\\\\4\\\\nlp.stanford.edu_IR-book_html_htmledition_references-and-further-reading-1.html',\n",
       " 'pa1-data\\\\4\\\\nlp.stanford.edu_IR-book_html_htmledition_the-search-user-experience-1.html',\n",
       " 'pa1-data\\\\4\\\\nlp.stanford.edu_IR-book_html_htmledition_vector-space-scoring-and-query-operator-interaction-1.html',\n",
       " 'pa1-data\\\\4\\\\nlp.stanford.edu_IR-book_html_htmledition_weighted-zone-scoring-1.html',\n",
       " 'pa1-data\\\\4\\\\nlp.stanford.edu_IR-book_html_htmledition_wildcard-queries-2.html',\n",
       " 'pa1-data\\\\4\\\\nlp.stanford.edu_IR-book_newslides.html',\n",
       " 'pa1-data\\\\6\\\\www-nlp.stanford.edu_IR-book_essir2011_',\n",
       " 'pa1-data\\\\8\\\\www.stanford.edu_class_cs124_']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BSBI_instance_compressed = BSBIIndex(data_dir='pa1-data', output_dir = 'output_dir_compressed', postings_encoding=CompressedPostings)\n",
    "BSBI_instance_compressed.retrieve('boolean retrieval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, let us now test on the dev queries to see if it works alright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results match for query: we are\n",
      "Results match for query: stanford class\n",
      "Results match for query: stanford students\n",
      "Results match for query: very cool\n",
      "Results match for query: the\n",
      "Results match for query: a\n",
      "Results match for query: the the\n",
      "Results match for query: stanford computer science\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir('dev_output_compressed')\n",
    "except:\n",
    "    pass\n",
    "for i in range(1, 9):\n",
    "    with open('dev_queries/query.' + str(i)) as q:\n",
    "        query = q.read()\n",
    "        my_results = [os.path.normpath(path) for path in BSBI_instance_compressed.retrieve(query)]\n",
    "        with open('dev_output_compressed/' + str(i) + '.out', 'wb') as o:\n",
    "            # reference_results = [os.path.normpath(x.strip()) for x in o.readlines()]\n",
    "            # assert my_results == reference_results, \"Results DO NOT match for query: \"+query.strip()\n",
    "            pkl.dump(my_results, o)\n",
    "        print(\"Results match for query:\", query.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code submission\n",
    "\n",
    "Now you are ready to submit the code part of your assignment. Refer to [submission instructions section](#Submission-instructions). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 其他压缩方法实现(选做)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement one more index compression method by filling in the `encode` and `decode` methods in `ECCompressedPostings`. \n",
    "\n",
    "Here are a few directions  \n",
    "1. [Section 5.4](https://nlp.stanford.edu/IR-book/pdf/05comp.pdf) in the textbook points to a few techniques (e.g., **gamma-encoding**) and related publications. \n",
    "2. You can also look into **Delta Encoding**, a compression technique very similar to Gamma Encoding, which can achieve an even better compression rate. \n",
    "3. Also, [ALGORITHM SIMPLE-9](https://github.com/manning/CompressionAlgorithms#simple-9) is an interesting encoding technique that you could also consider implementing.\n",
    "\n",
    "You should store each postings list is stored in a multiple of bytes (rather than bits). The positions and lengths in index file are in multiples of bytes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting submission/ec_compressed_postings.py\n"
     ]
    }
   ],
   "source": [
    "%%tee submission/ec_compressed_postings.py\n",
    "class ECCompressedPostings:\n",
    "    ### gamma-encoding\n",
    "    #If you need any extra helper methods you can add them here \n",
    "    ### Begin your code\n",
    "    @staticmethod\n",
    "    def getLength(number):\n",
    "        length = 0\n",
    "        while number > (2 ** length):\n",
    "            length += 1\n",
    "        if number == 2 ** length:\n",
    "            return length\n",
    "        return length - 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def getOffset(number, length):\n",
    "        return number - 2 ** length\n",
    "    \n",
    "    @staticmethod\n",
    "    def convertDecimal(length, offset):\n",
    "        coefficient = 2 ** (length + 1)\n",
    "        lengthDecimal = 2 ** length - 1\n",
    "        return coefficient * lengthDecimal + offset\n",
    "        \n",
    "    @staticmethod    \n",
    "    def prepand(bts, decimal):\n",
    "        bt = decimal.to_bytes(length=1, byteorder='little')\n",
    "        if bts is None:\n",
    "            return bt\n",
    "        return bt + bts\n",
    "    \n",
    "    @staticmethod\n",
    "    def encodeDecimal(decimal):\n",
    "        bts = None\n",
    "        while decimal >= 256:\n",
    "            bts = ECCompressedPostings.prepand(bts, decimal % 256)\n",
    "            decimal = decimal // 256\n",
    "        return ECCompressedPostings.prepand(bts, decimal)\n",
    "        \n",
    "    @staticmethod\n",
    "    def encode_number(number):\n",
    "        length = ECCompressedPostings.getLength(number)\n",
    "        offset = ECCompressedPostings.getOffset(number, length)\n",
    "        decimal = ECCompressedPostings.convertDecimal(length, offset)\n",
    "        bts = ECCompressedPostings.encodeDecimal(decimal)\n",
    "        return bts\n",
    "    \n",
    "    @staticmethod\n",
    "    def decodeDelimiter(number, status):\n",
    "        if status == 1:\n",
    "            i = 7\n",
    "            ret, flag = 0, 0\n",
    "            while i >= 0:\n",
    "                if flag == 0 and number & (2 ** i) != 0:\n",
    "                    ret += 1\n",
    "                    flag = 1\n",
    "                    start_point = i\n",
    "                elif flag == 1 and number & (2 ** i) != 0:\n",
    "                    ret += 1\n",
    "                elif flag == 1 and number & (2 ** i) == 0:\n",
    "                    return start_point, ret\n",
    "                i -= 1\n",
    "            else:\n",
    "                raise ValueError('number is \\x00, should not occur')\n",
    "        for i in range(7, -1, -1):\n",
    "            if number & (2 ** i) == 0:\n",
    "                return 7, 7 - i\n",
    "        else:\n",
    "            raise ValueError('number is \\xff, no delimiter found')\n",
    "            \n",
    "    @staticmethod\n",
    "    def decodeOffset(encoded_postings_list, start_point, resLen, length, currentIdx):\n",
    "        resOffset = encoded_postings_list[currentIdx] & (2 ** (start_point - resLen) - 1)\n",
    "        offsetLen = length + resLen - 7\n",
    "        if start_point != 7:\n",
    "            offsetLen = resLen\n",
    "        # assert offsetLen % 8 == 0, \"offsetLen is {}\".format(offsetLen)\n",
    "        exceedIdx = offsetLen // 8\n",
    "        offset = resOffset\n",
    "        for i in range(1, exceedIdx+1):\n",
    "            offset = offset * 256 + encoded_postings_list[currentIdx + i]\n",
    "        return offset, currentIdx + exceedIdx + 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def decodeDecimal(length, offset):\n",
    "        return 2 ** length + offset\n",
    "    \n",
    "    @staticmethod\n",
    "    def fillGap(numbers):\n",
    "        newNumbers = []\n",
    "        base = 0\n",
    "        for gap in numbers:\n",
    "            base = base + gap\n",
    "            newNumbers.append(base)\n",
    "        return newNumbers\n",
    "    ### End your code\n",
    "    \n",
    "    @staticmethod\n",
    "    def encode(postings_list):\n",
    "        \"\"\"Encodes `postings_list` \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        postings_list: List[int]\n",
    "            The postings list to be encoded\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        bytes: \n",
    "            Bytes reprsentation of the compressed postings list \n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        beforehand = 0\n",
    "        bytestream = None\n",
    "        for number in postings_list:\n",
    "            gap = number - beforehand\n",
    "            encodedBytes = ECCompressedPostings.encode_number(gap)\n",
    "            if bytestream is None:\n",
    "                bytestream = encodedBytes\n",
    "            else:\n",
    "                bytestream += encodedBytes\n",
    "            beforehand = number\n",
    "        return bytestream\n",
    "        ### End your code\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def decode(encoded_postings_list):\n",
    "        \"\"\"Decodes a byte representation of compressed postings list\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        encoded_postings_list: bytes\n",
    "            Bytes representation as produced by `CompressedPostings.encode` \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        List[int]\n",
    "            Decoded postings list (each posting is a docId)\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        from math import log\n",
    "        numbers = []\n",
    "        length = 0\n",
    "        i = 0\n",
    "        status = 1\n",
    "        while i < len(encoded_postings_list):\n",
    "            if encoded_postings_list[i] == 0:\n",
    "                numbers.append(1)\n",
    "                i += 1\n",
    "                continue\n",
    "            temp = log((encoded_postings_list[i] + 1), 2)\n",
    "            if int(temp) == temp:\n",
    "                length += ECCompressedPostings.getLength(encoded_postings_list[i] + 1)\n",
    "                i += 1\n",
    "                status = 2\n",
    "            else:\n",
    "                start_point, resLen = ECCompressedPostings.decodeDelimiter(encoded_postings_list[i], status)\n",
    "                length += resLen\n",
    "                offset, i = ECCompressedPostings.decodeOffset(encoded_postings_list, start_point, resLen, length, i)\n",
    "                number = ECCompressedPostings.decodeDecimal(length, offset)\n",
    "                numbers.append(number)\n",
    "                length = 0\n",
    "                status = 1\n",
    "        numbers = ECCompressedPostings.fillGap(numbers)\n",
    "        return numbers\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x00\\x00\\x7f\\xff\\xff\\x06A\\xe6\\x1f\\xff\\xff\\xff\\x81\\xcf;\\xd0'\n",
      "[1, 2, 8798696, 1112899000]\n"
     ]
    }
   ],
   "source": [
    "### Begin your code\n",
    "postings_list = [1, 2, 8798696, 1112899000]\n",
    "a = ECCompressedPostings.encode(postings_list)\n",
    "print(a)\n",
    "b = ECCompressedPostings.decode(a)\n",
    "print(b)\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write a description of your index compression method as well as a discussion of the space/speed trade-off of this additional compression method, in comparison to what we implemented in previous tasks.**\n",
    "\n",
    "> Gamma-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "799996\n",
      "length:799996, 0.013950347900390625s for unpressed coding\n",
      "588549\n",
      "length:588549, 2.571326494216919s for variable coding\n",
      "200084\n",
      "length:200084, 1.0661489963531494s for gamma coding\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "postings_list = []\n",
    "number = 0\n",
    "for j in range(1, 200000):\n",
    "    if j > 100:\n",
    "        j = 1\n",
    "    number += j\n",
    "    postings_list.append(number)\n",
    "\n",
    "# uncompressed\n",
    "start = time.time()\n",
    "unpressed = UncompressedPostings.encode(postings_list)\n",
    "length_unpressed = len(unpressed)\n",
    "assert postings_list == UncompressedPostings.decode(unpressed)\n",
    "end = time.time()\n",
    "print(length_unpressed)\n",
    "print('length:{}, {}s for unpressed coding'.format(length_unpressed, end-start))\n",
    "\n",
    "# variable coding\n",
    "start = time.time()\n",
    "unpressed = CompressedPostings.encode(postings_list)\n",
    "length_unpressed = len(unpressed)\n",
    "assert postings_list == CompressedPostings.decode(unpressed)\n",
    "end = time.time()\n",
    "print(length_unpressed)\n",
    "print('length:{}, {}s for variable coding'.format(length_unpressed, end-start))\n",
    "\n",
    "# gamma coding\n",
    "start = time.time()\n",
    "unpressed = ECCompressedPostings.encode(postings_list)\n",
    "length_unpressed = len(unpressed)\n",
    "assert postings_list == ECCompressedPostings.decode(unpressed)\n",
    "end = time.time()\n",
    "print(length_unpressed)\n",
    "print('length:{}, {}s for gamma coding'.format(length_unpressed, end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
